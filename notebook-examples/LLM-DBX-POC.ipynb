{"cells":[{"cell_type":"markdown","source":["# The Use Case\n\n## For The Business\nFor this proof of concept (POC), we wanted to use an actual use case from our HR department. In fact, the exact ask was \"Imagine if we flipped our HR inbox/etc. into something similar like a bot…trained it with our handbook, IT policies, benefits, engagement protocol, etc…it would cut down on so many of the asks!\"\n\n## On The Tech Side\nTo keep the data (in this case, our employee handbook) within our Azure and Databricks environments, we are used an open source foundational LLM. We wanted to test out running a POC on a ridiculously small Databricks cluster...so yeah 15GB, CPU based, with the 13.1 ML Runtime. #jarvisTakeTheWheel\n\n## Final Note\nWhile this is functional, it is a proof of concept...**raw and uncut**. It's meant to showcase a basic LLM solution approach within Databricks and accelerate your LLM experimentation efforts. Despite its miniscule size (don't judge a notebook by its size ;)) there is a fair amount of exploration and experimentation that went into this POC.  \n\nLet's get into it."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ced0a32-7a12-4fe6-8885-0407dd79b0a7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Proof of Concept, Iteration 1: Start"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc317b77-c70b-4b87-8860-585a3270bc8b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install azure-storage-blob langchain transformers unstructured lancedb bitsandbytes einops safetensors"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fa3d263a-8dec-428d-a70d-e61323f09585","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.library.restartPython()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a4d23b56-a915-45f4-a2e3-94d9b1d700bd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load & Chunk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bb274c90-9185-4824-a512-7495856505d8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Get the Azure blob storage keys from the container under Security + networking -> Access keys\n# Using Langchain to load our pdf\nfrom langchain.document_loaders import AzureBlobStorageFileLoader\nfrom langchain.text_splitter import TokenTextSplitter\n\nloader = AzureBlobStorageFileLoader(\n    conn_str=\"[your connection string here]\",\n    container=\"[name of blob container here]\",\n    blob_name=\"[name of the handbook (in pdf format) uploaded to the blob container above]\",\n)\n\nhandbook = loader.load()\ntext_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=0)\nchunks = text_splitter.split_documents(handbook)\n\n# Result is LangChain document objects with a token size around what you defined in chunk_size\nprint (\"Your handbook has been chunked into\", len(chunks), \"documents. \")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d1e59dcf-83d0-4e5f-86bd-571eb055a951","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Time For Embedding"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1d2d7de9-8b7c-4a76-b83b-9c54081f7207","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Need this for a local pipeline wrapper around the model rather than using a hosted model on Hugging Face Hub\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Download model from Hugging face\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"77df889f-84ba-45fe-847e-a0930f1d23c2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Let's do some vectorization. (It's a Lance...hello)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b1fb72da-1a44-42ab-a2ee-91bd70f834b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from langchain.vectorstores import LanceDB\n\nimport lancedb\n\ndb = lancedb.connect(\"/tmp/lancedb\")\ntable = db.create_table(\n    \"handbook_table\",\n    data=[\n        {\n            \"vector\": embeddings.embed_query(\"HR\"),\n            \"text\": \"Hello HR\",\n            \"id\": \"1\",\n        }\n    ],\n    mode=\"overwrite\",\n)\n\ndocsearch = LanceDB.from_documents(chunks, embeddings, connection=table)\n\ndef get_similar_docs(question, similar_doc_count):\n  docs = docsearch.similarity_search(question, k=similar_doc_count)\n  return docs\n\nget_similar_docs(\"How much PTO do I get in my first year?\", 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6292ef7c-c8fc-4180-a4f0-21813c911049","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Proof of Concept, Iteration 1: Done\nOk, so what we have demonstrated is we can take an employee handbook, in pdf format, and ask questions that would be answered by that handbook. At this point, we could consider the first iteration of a proof of concept (POC) complete. It's definitely rough around the edges but the whole point was to proof out the initial concept. \n\n## The Learnings\nThe runtime selected in Databricks matters.\n\nIn a number of cases, using a LLM isn't absolute necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51c55889-d11d-4b49-89b4-6f1f9e286b4d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Proof of Concept, Iteration 2: Start\n\nAfter some initial feedback and review of the first iteration of the POC, the request was made for another iteration  to make the responses a bit more \"human understandable\". Now comes the language model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75f411a3-7ccb-4cc3-b0af-506819ac8792","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Load a language model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccb14177-6254-4678-b7fe-880faf4dedbd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import torch\nfrom transformers import GPT2Tokenizer, GPT2Model, AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom transformers import TFAutoModelForCausalLM\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\nmodel2 = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"834c13cc-70bd-4d74-bbd3-827318008008","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Configure our pipeline\nHere we are configuring our transformers pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"926dac75-c828-4aa9-9b16-334c88edb910","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from transformers import pipeline\n\npipeline2 = pipeline(\n        model=\"distilgpt2\",\n        use_cache=True,\n        device_map=\"auto\",\n        max_new_tokens=500,\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ce524cad-5a21-453a-98a7-0c8784a64cd1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create and run our chain"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2de6bdc1-9a5b-4c66-b569-5a4f4c22fb0f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Yet Another Test\nfrom langchain.chains import RetrievalQA\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains.question_answering import load_qa_chain\n\nquery = \"How much PTO do I get in my first year?\"\n\n# create a chain to answer questions \nqa = RetrievalQA.from_chain_type(\n    llm=HuggingFacePipeline(pipeline=pipeline2), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n\nresult = qa.run(query)\nprint(\"The question was\", query, \"The answer is\", result)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7a6a403e-a858-4081-abb9-a92733d9d27b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Proof of Concept, Iteration 2: Done\n\n## The Learnings\n\n### Model selection matters. \nWe tested a fair number of models, including but not limited to: \n- dolly (I mean, we had to right?)\n- tinyroberta-squad2 (we needed a hyperparameter with a max_seq_len of 2000+)\n- mpt-7b\n- falcon-7b-instruct (alot of python kernel timeouts with this one)\n- gpt2\n\nRemember this: Task selection is an important consideration when choose the model\n\n**Performance**\n\nAccording to the creators of the model we used ( [GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md)), \"Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\"\n\nIn this POC, you will see that the model does return back results...but it also peppers its response with alot of...let's call it mumbo jumbo. Others may call it hallucinations.\n\nBelow we commented out one approach we tried, which was quantizizing a model to 4bits. We achieved moderate results but we found ourselves distracted by twisting alot of knobs and dials. For the sake of speed to value from the POC, we pivoted. But it's an approach we will cover in another notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50c8c7dc-d997-446b-a27d-651d2e1f28df","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#CONFIGURE THE QUANTIZATION OF MODEL AND LOAD IT\n#Keeping this block in here to showcase a bit of the experimentation during this iteration. One of the attempts made to run the POC on low-end resources was to quantizize a model to 4 bits. \n\n\n#import torch\n#from transformers import BitsAndBytesConfig\n#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n#quantization_config = BitsAndBytesConfig(\n#    load_in_4bit=True,\n#    bnb_4bit_compute_dtype=torch.float16,\n#    bnb_4bit_quant_type=\"nf4\",\n#    bnb_4bit_use_double_quant=True,\n#)\n\n#model_id = \"ComCom/gpt2-small\"\n\n#tokenizer = AutoTokenizer.from_pretrained(model_id)\n#model_4bit = AutoModelForCausalLM.from_pretrained(\n#        model_id, \n#        device_map=\"auto\",\n#        offload_folder=\"offload\",\n#        quantization_config=quantization_config,\n#        trust_remote_code=True)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3ff472ef-bb18-4251-adac-b97662cc1ca8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"LLM-DBX-POC","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"widgetLayout":[]},"language":"python","widgets":{"reset_vector_database":{"nuid":"890782d9-f81e-485b-96ee-24fe415c4e9b","currentValue":"false","widgetInfo":{"widgetType":"dropdown","name":"reset_vector_database","defaultValue":"false","label":"Recompute embeddings for chromadb","options":{"widgetType":"dropdown","choices":["false","true"]}}}}}},"nbformat":4,"nbformat_minor":0}
