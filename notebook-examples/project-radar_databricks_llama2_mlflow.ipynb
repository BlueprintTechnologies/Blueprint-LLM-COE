{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9feb32e-3acf-4acd-8581-b2c1cd9196d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Project Radar: LLM Solution on Databricks\n",
    "\n",
    "#### Persona \n",
    "You are a ML Engineer for a mid-size technology consultancy (Company).\n",
    "\n",
    "#### Scenario\n",
    "Project managers in the Company tried ChatGPT. They went to the Managing Director of Professional Services for the Company and said \"We want ChatGPT for our engagements\".\n",
    "\n",
    "#### Use Case\n",
    "Project managers in the professional services division of the Company want to enable their customers to ask questions about their specific project using data from the project status report. However, the leadership for the Company doesn't want the project data sent to OpenAI (ChatGPT). As a ML Engineer, you have to create a Large Language Model (LLM) solution within Azure and Databricks that avoids the use of third party LLM services.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "\n",
    "1. Utilize open-source Llama 2 instead of OpenAI's GPT LLM.\n",
    "2. Serve the model for inference via Databricks [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) via some MLFlow goodness.\n",
    "\n",
    "\n",
    "##### About Llama2\n",
    "*_Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks tested, and in human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM._*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268d92d3-7495-4ffd-b21a-1f2455ead321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# I. Prerequisites\n",
    "\n",
    "Environment for this notebook:\n",
    "- Runtime: 13.2 GPU ML Runtime\n",
    "- Instance: Recommended - `Standard_NC6s_v3` on Azure. *_This was also tested on `Standard_DS3_v2` (non-GPU)_*\n",
    "\n",
    "LLM needs\n",
    "1. Request access from [Meta](https://ai.meta.com/resources/models-and-libraries/llama-downloads)\n",
    "2. Accept TOS on Hugging Face [repo](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n",
    "\n",
    "Estimated time to run: ~6 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1740d2-6e04-4242-a37c-220cd074d76e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## A. Install some stuff\n",
    "\n",
    "*_Time to run: ~30 seconds_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e37b5f-5c95-4c9c-813f-96a1df4f6447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow-skinny[databricks]>=2.4.1\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf43b5ad-7d2d-40d6-b1b9-d16607b0189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347168dc-6980-493b-8df0-baa474b0c2ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Enter your Hugging Face token. Get it from https://huggingface.co/settings/tokens\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ba6b91-571d-4958-93a6-2ba9fefde877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## B. Download Llama2\n",
    "\n",
    "*_Time to run: ~3 - 5 minutes (depending on internet connection)._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f299bdc-fbe7-4456-aa53-e0ee8ea3c133",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# If the model has been downloaded in previous cells, this will not repetitively download large model files, but only the remaining files in the repo\n",
    "snapshot_location = snapshot_download(repo_id=model, revision=revision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f721bcd-8e31-472f-99f5-29c1e9cb2846",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# II. MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d13e05c-e07e-4124-b972-c019f10ed44c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## A. Define the Model\n",
    "*_Time to run: ~ 30 seconds_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3156e152-3fdb-4f76-8ab3-b9f9056b5714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Define prompt template. This is straight from the Meta repo https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# Define PythonModel to log with mlflow.pyfunc.log_model https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html\n",
    "\n",
    "class Llama2(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model repository.\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer and language model\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            context.artifacts['repository'], padding_side=\"left\")\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts['repository'], \n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True, \n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            pad_token_id=self.tokenizer.eos_token_id)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _build_prompt(self, instruction):\n",
    "        \"\"\"\n",
    "        This method generates the prompt for the model.\n",
    "        \"\"\"\n",
    "        return f\"\"\"<s>[INST]<<SYS>>\\n{DEFAULT_SYSTEM_PROMPT}\\n<</SYS>>\\n\\n\\n{instruction}[/INST]\\n\"\"\"\n",
    "\n",
    "    def _generate_response(self, prompt, temperature, max_new_tokens):\n",
    "        \"\"\"\n",
    "        This method generates prediction for a single input.\n",
    "        \"\"\"\n",
    "        # Build the prompt\n",
    "        prompt = self._build_prompt(prompt)\n",
    "\n",
    "        # Encode the input and generate prediction\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "        output = self.model.generate(encoded_input, do_sample=True, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "        # Decode the prediction to text\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Removing the prompt from the generated text\n",
    "        prompt_length = len(self.tokenizer.encode(prompt, return_tensors='pt')[0])\n",
    "        generated_response = self.tokenizer.decode(output[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        return generated_response\n",
    "      \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(len(model_input)):\n",
    "          prompt = model_input[\"prompt\"][i]\n",
    "          temperature = model_input.get(\"temperature\", [1.0])[i]\n",
    "          max_new_tokens = model_input.get(\"max_new_tokens\", [100])[i]\n",
    "\n",
    "          outputs.append(self._generate_response(prompt, temperature, max_new_tokens))\n",
    "      \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a896cf-26d8-4554-936c-832801cbf852",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## B. Run experiment\n",
    "Outcomes \n",
    "1. Check out Experiments...you'll see this notebook logged as an experiment run.\n",
    "2. Check out Models...you'll see a model has been registered named \"llama2-4-u\"\n",
    "\n",
    "NOTE: Wait time for model registration may exceed the max depending on your cluster environment. Monitor accordingly.\n",
    "\n",
    "*_Time to run: ~5 minutes_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b419c62f-93df-41ef-b813-c1aa5e063688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema([\n",
    "    ColSpec(DataType.string, \"prompt\"), \n",
    "    ColSpec(DataType.double, \"temperature\"), \n",
    "    ColSpec(DataType.long, \"max_new_tokens\")])\n",
    "output_schema = Schema([ColSpec(DataType.string)])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "registered_model_name=\"llama2-4-u\"\n",
    "\n",
    "# Define input example\n",
    "input_example=pd.DataFrame({\n",
    "            \"prompt\":[\"what is Databricks?\"], \n",
    "            \"temperature\": [0.5],\n",
    "            \"max_new_tokens\": [100]})\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run() as run:  \n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=Llama2(),\n",
    "        artifacts={'repository' : snapshot_location},\n",
    "        pip_requirements=[\"torch\", \"transformers\", \"accelerate\"],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=registered_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cff631e-2975-4cb1-8c8e-e656f717f4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# III. CHOOSE YOUR OWN ADVENTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afad9f1f-7476-4252-920a-26386d2a9405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Path 1. Register Model with Unity Catalog Then Serve\n",
    "\n",
    " By default, MLflow registers models in the Databricks workspace Models registry. However, we are going to register Llama2 in Unity Catalog instead. Models in Unity Catalog extends the benefits of Unity Catalog to ML models, including centralized access control, auditing, lineage, and model discovery across workspaces.\n",
    "\n",
    "Key features of Models in Unity Catalog include:\n",
    "\n",
    "- Namespacing and governance for models, so you can group and govern models at the environment, project, or team level.\n",
    "\n",
    "- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n",
    "\n",
    "- Model versioning.\n",
    "\n",
    "- Model deployment via aliases.\n",
    "\n",
    "*_Time to run: ~55 minutes_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6eb9535-46d9-4d31-b461-3e3319c96d02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A. Prerequisites\n",
    "Create a new Unity Catalog named `models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c314fd86-1578-4ff9-bad1-45b3470e2f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure MLflow Python client to register model in Unity Catalog\n",
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99a805a-4266-4e2f-bf19-1528c2474a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The UC registered model name follows the pattern <catalog_name>.<schema_name>.<model_name>\n",
    "registered_name = \"models.default.llamav2_7b_chat_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5dd669-299e-4adb-af17-9ccbf097e8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### B. Register model\n",
    "*_Time to run: 5 - 20 minutes (based on your cluster)_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff9d631-bc14-4dd8-994b-1eaf59f2829c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = mlflow.register_model(\n",
    "    \"runs:/\"+run.info.run_id+\"/model\",\n",
    "    registered_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121dfb1f-0cd1-4704-866c-e448fd5e01f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Let's register and give the model an alias\n",
    "client.set_registered_model_alias(name=registered_name, alias=\"Champion\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1203362b-b11f-407f-a21e-a6bbda09cbc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### C. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a2e33d-3c17-42a3-8c1b-abce41e18b37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{registered_name}@Champion\")\n",
    "\n",
    "# Make a prediction using the loaded model\n",
    "loaded_model.predict(\n",
    "    {\n",
    "        \"prompt\": [\"What is ML?\", \"What is large language model?\"],\n",
    "        \"temperature\": [0.1, 0.5],\n",
    "        \"max_new_tokens\": [100, 100],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2258955a-3a4f-4faf-90c9-74e24b975a38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### D. Databricks Model Serving from Unity Catalog model\n",
    "Create a Databricks GPU Model Serving Endpoint that serves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e7e375-bfdf-4a49-becf-d16f853f400f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Provide a name to the serving endpoint\n",
    "endpoint_name = 'llama2-7b-chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ee0d39-537b-40c0-a11a-99b8bf9b5768",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "print('URL:', databricks_url)\n",
    "print('token:', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd88f233-466f-4855-a311-240a1990c6ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "deploy_headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "deploy_url = f'{databricks_url}/api/2.0/serving-endpoints'\n",
    "\n",
    "model_version = result  # the returned result of mlflow.register_model\n",
    "endpoint_config = {\n",
    "  \"name\": endpoint_name,\n",
    "  \"config\": {\n",
    "    \"served_models\": [{\n",
    "      \"name\": f'{model_version.name.replace(\".\", \"_\")}_{model_version.version}',\n",
    "      \"model_name\": model_version.name,\n",
    "      \"model_version\": model_version.version,\n",
    "      \"workload_type\": \"GPU_MEDIUM\",\n",
    "      \"workload_size\": \"Small\",\n",
    "      \"scale_to_zero_enabled\": \"False\"\n",
    "    }]\n",
    "  }\n",
    "}\n",
    "endpoint_json = json.dumps(endpoint_config, indent='  ')\n",
    "\n",
    "# Send a POST request to the API\n",
    "deploy_response = requests.request(method='POST', headers=deploy_headers, url=deploy_url, data=endpoint_json)\n",
    "\n",
    "if deploy_response.status_code != 200:\n",
    "  raise Exception(f'Request failed with status {deploy_response.status_code}, {deploy_response.text}')\n",
    "\n",
    "# Show the response of the POST request\n",
    "# When first creating the serving endpoint, it should show that the state 'ready' is 'NOT_READY'\n",
    "# You can check the status on the Databricks model serving endpoint page, it is expected to take ~35 min for the serving endpoint to become ready\n",
    "print(deploy_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90622372-4328-46c3-8b03-8cb47f734a02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Path 2: Straight to Serve\n",
    "\n",
    "Model has previously been registered in the Models registry. Let's get straight to creating a Serving endpoint for model inference.\n",
    "\n",
    "Time to run: ~1 minute (registration time ~45 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65be8bb0-eae6-4026-ac97-b957450b583f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Provide a name to the serving endpoint\n",
    "endpoint_name = 'llama2-7b-chat'\n",
    "model_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eaef934-8c57-4bc6-adef-51f105311e27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43b223a-a6a5-46aa-b47d-bfa7feda1568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "deploy_headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "deploy_url = f'{databricks_url}/api/2.0/serving-endpoints'\n",
    "\n",
    "endpoint_config = {\n",
    "  \"name\": endpoint_name,\n",
    "  \"config\": {\n",
    "    \"served_models\": [{\n",
    "      \"name\": registered_model_name,\n",
    "      \"model_name\": registered_model_name,\n",
    "      \"model_version\": model_version,\n",
    "      \"workload_size\": \"Small\",\n",
    "      \"scale_to_zero_enabled\": \"False\"\n",
    "    }]\n",
    "  }\n",
    "}\n",
    "endpoint_json = json.dumps(endpoint_config, indent='  ')\n",
    "\n",
    "# Register with Model Serving\n",
    "deploy_response = requests.request(method='POST', headers=deploy_headers, url=deploy_url, data=endpoint_json)\n",
    "\n",
    "if deploy_response.status_code != 200:\n",
    "  raise Exception(f'Request failed with status {deploy_response.status_code}, {deploy_response.text}')\n",
    "\n",
    "# Show response\n",
    "print(deploy_response.json())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project-radar_databricks_llama2_mlflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
